# 目录结构（建议直接照抄）

```
va/
├─ CMakeLists.txt
├─ config/                  # 上条消息给过 YAML，可直接复用
│   ├─ app.yaml
│   ├─ profiles.yaml
│   ├─ models.yaml
│   └─ analyzer_params.yaml
└─ src/
    ├─ core/
    │   ├─ utils.hpp
    │   ├─ engine_manager.hpp
    │   ├─ factories.hpp
    │   ├─ pipeline.hpp           ┐
    │   ├─ pipeline.cpp           │ 数据面：Source→Decode→Analyzer→Encode→WHIP
    │   ├─ pipeline_builder.hpp   │
    │   ├─ pipeline_builder.cpp   ┘
    │   ├─ track_manager.hpp
    │   └─ track_manager.cpp
    ├─ analyzer/
    │   ├─ interfaces.hpp         # IPreprocessor/IModelSession/IPostprocessor/IRenderer
    │   ├─ analyzer.hpp           # 外壳：原子热插拔
    │   ├─ preproc_letterbox_cpu.hpp
    │   ├─ preproc_letterbox_cpu.cpp
    │   ├─ preproc_letterbox_cuda.hpp    # 可选：CUDA/NPP 预处理
    │   ├─ preproc_letterbox_cuda.cpp    # （以后接 IO 绑定零拷贝）
    │   ├─ ort_session.hpp
    │   ├─ ort_session.cpp
    │   ├─ postproc_yolo_det.hpp
    │   ├─ postproc_yolo_det.cpp
    │   ├─ postproc_yolo_seg.hpp
    │   ├─ postproc_yolo_seg.cpp
    │   ├─ postproc_detr.hpp
    │   └─ postproc_detr.cpp
    ├─ media/
    │   ├─ source.hpp             # IFrameSource/ISwitchableSource
    │   ├─ source_switchable_rtsp.hpp
    │   ├─ source_switchable_rtsp.cpp     # FFmpeg 解码(send/receive)
    │   ├─ encoder.hpp
    │   ├─ encoder_h264_ffmpeg.hpp
    │   ├─ encoder_h264_ffmpeg.cpp        # FFmpeg 编码(send/receive)
    │   ├─ transport.hpp
    │   ├─ transport_whip.hpp
    │   └─ transport_whip.cpp             # WHIP 发布
    ├─ server/
    │   └─ rest.cpp               # /subscribe /unsubscribe /switch 等
    └─ composition_root.cpp       # 组合根：把具体实现绑定到工厂（显式，无反射）
```

------

# 核心数据与公共工具（`core/utils.hpp`）

```
#pragma once
#include <vector>
#include <cstdint>
#include <string>
#include <memory>

struct Frame {
  int w=0, h=0;
  double pts_ms=0;
  std::vector<uint8_t> bgr; // 简化为BGR24；后续可扩为NV12/GPU句柄
};

struct LetterboxMeta {
  float scale=1.f; int pad_x=0, pad_y=0; int in_w=0, in_h=0;
};

enum class DType { U8, F32, F16 };
struct TensorView {
  void* data=nullptr;
  std::vector<int64_t> shape;
  DType dtype=DType::F32;
  bool on_gpu=false;
};

struct Box { float x1,y1,x2,y2,score; int cls; };

struct ModelOutput {
  std::vector<Box> boxes;
  std::vector<std::vector<uint8_t>> masks; // 可选：实例分割
};

inline double ms_now();
```

------

# Analyzer 模块：接口与外壳

## 接口（`analyzer/interfaces.hpp`）

```
#pragma once
#include "core/utils.hpp"
#include <vector>

struct IPreprocessor {
  virtual ~IPreprocessor()=default;
  virtual bool run(const Frame& in, TensorView& out, LetterboxMeta& meta)=0;
};

struct IModelSession {
  virtual ~IModelSession()=default;
  // 约定：支持 GPU I/O（配合 ORT IOBinding）
  virtual bool run(const TensorView& x, std::vector<TensorView>& ys)=0;
};

struct IPostprocessor {
  virtual ~IPostprocessor()=default;
  virtual bool run(const std::vector<TensorView>& raw, const LetterboxMeta& m, ModelOutput& out)=0;
};

struct IRenderer {
  virtual ~IRenderer()=default;
  virtual bool draw(const Frame& in, const ModelOutput& mo, Frame& out)=0;
};

// 用于 Pipeline 的统一接口
struct IFrameFilter {
  virtual ~IFrameFilter()=default;
  virtual bool process(const Frame& in, Frame& out)=0;
};
```

## 外壳（`analyzer/analyzer.hpp`）

```
#pragma once
#include "interfaces.hpp"
#include <atomic>
#include <memory>

struct AnalyzerParams { float conf=0.25f, iou=0.50f; float mask_thresh=0.5f; /*classes, roi...*/ };

class Analyzer final : public IFrameFilter {
public:
  bool process(const Frame& in, Frame& out) override {
    auto pre  = pre_.load(std::memory_order_acquire);
    auto ses  = ses_.load(std::memory_order_acquire);
    auto post = post_.load(std::memory_order_acquire);
    auto ren  = ren_.load(std::memory_order_acquire);
    if (!pre || !ses || !post) { out = in; return true; }

    TensorView x; LetterboxMeta m{};
    if (!pre->run(in, x, m)) return false;

    std::vector<TensorView> ys;
    if (!ses->run(x, ys)) return false;

    ModelOutput mo; if (!post->run(ys, m, mo)) return false;
    if (ren) return ren->draw(in, mo, out);
    out = in; return true;
  }

  // 热插拔：换好+预热后再 store
  void swap_pre (std::shared_ptr<IPreprocessor> p){ pre_.store(std::move(p), std::memory_order_release); }
  void swap_ses (std::shared_ptr<IModelSession> s){ ses_.store(std::move(s), std::memory_order_release); }
  void swap_post(std::shared_ptr<IPostprocessor> p){ post_.store(std::move(p), std::memory_order_release); }
  void swap_ren (std::shared_ptr<IRenderer> r){ ren_.store(std::move(r), std::memory_order_release); }
  void set_params(std::shared_ptr<AnalyzerParams> p){ params_.store(std::move(p), std::memory_order_release); }
  std::shared_ptr<AnalyzerParams> params() const { return params_.load(std::memory_order_acquire); }

private:
  std::atomic<std::shared_ptr<IPreprocessor>>  pre_{nullptr};
  std::atomic<std::shared_ptr<IModelSession>>  ses_{nullptr};
  std::atomic<std::shared_ptr<IPostprocessor>> post_{nullptr};
  std::atomic<std::shared_ptr<IRenderer>>      ren_{nullptr};
  std::atomic<std::shared_ptr<AnalyzerParams>> params_{std::make_shared<AnalyzerParams>()};
};
```

## 预处理（CPU/可选CUDA）

`preproc_letterbox_cpu.*`（CPU 版示例；CUDA 版接口相同，内部用 NPP/自研核后直接产 GPU Tensor，配合 ORT **IOBinding** 零拷贝）[ONNX Runtime](https://onnxruntime.ai/docs/performance/tune-performance/iobinding.html?utm_source=chatgpt.com)

```
// preproc_letterbox_cpu.hpp
#pragma once
#include "interfaces.hpp"
class LetterboxPreCPU final : public IPreprocessor {
public:
  LetterboxPreCPU(int in_w, int in_h): in_w_(in_w), in_h_(in_h) {}
  bool run(const Frame& in, TensorView& out, LetterboxMeta& meta) override;
private:
  int in_w_, in_h_;
};
// preproc_letterbox_cpu.cpp
#include "preproc_letterbox_cpu.hpp"
bool LetterboxPreCPU::run(const Frame& in, TensorView& out, LetterboxMeta& meta) {
  // 1) 计算 scale 与 pad_x/pad_y （保持纵横比）
  // 2) BGR→RGB、归一化、Letterbox 到 in_w_ x in_h_，排布为 NCHW float
  // 3) 填 out: data/shape={1,3,in_h_,in_w_}/dtype=F32/on_gpu=false
  // 4) 填 meta: {scale,pad_x,pad_y,in_w,in_h}
  return true;
}
```

## 推理会话（ONNX Runtime，EP + IOBinding）

`ort_session.*`（按全局 EngineSpec 追加 **CUDA** 或 **TensorRT** EP；使用 **IoBinding** 绑定输入输出，Run 时不做隐式拷贝）[ONNX Runtime+3ONNX Runtime+3ONNX Runtime+3](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html?utm_source=chatgpt.com)

```
// ort_session.hpp
#pragma once
#include "interfaces.hpp"
#include "core/utils.hpp"
#include "core/engine_manager.hpp"
#include <onnxruntime_cxx_api.h>

class OrtSession final : public IModelSession {
public:
  OrtSession(const std::string& onnx, const EngineSpec& spec, int in_w, int in_h);
  bool run(const TensorView& x, std::vector<TensorView>& ys) override;
private:
  Ort::Env env_{ORT_LOGGING_LEVEL_WARNING, "va-ort"};
  Ort::Session sess_{nullptr};
  // 可缓存 IoBinding/Value，用于复用 GPU 缓冲
};
// ort_session.cpp
#include "ort_session.hpp"
OrtSession::OrtSession(const std::string& onnx, const EngineSpec& spec, int, int) {
  Ort::SessionOptions so;
  if (spec.ep == "ort-cuda") {
    OrtCUDAProviderOptions opts{}; opts.device_id = spec.device;
    OrtSessionOptionsAppendExecutionProvider_CUDA(so, opts);           // CUDA EP
  } else if (spec.ep == "ort-tensorrt") {
    // OrtSessionOptionsAppendExecutionProvider_TensorRT(so, trt_opts); // TensorRT EP
  }
  sess_ = Ort::Session(env_, onnx.c_str(), so);
  // TODO: 创建 Ort::IoBinding 并预绑定输入/输出缓冲（GPU），降低拷贝成本
}

bool OrtSession::run(const TensorView& x, std::vector<TensorView>& ys) {
  // if (x.on_gpu) 走 IoBinding；否则构造 CPU Ort::Value，常规 Run()
  // 将输出包装为 TensorView（可在 GPU 上）
  return true;
}
```

## 后处理（YOLO/SEG/DETR）

YOLO：阈值（`conf`）与 **NMS 的 IoU** 是推理质量的重要旋钮（Ultralytics 文档有明确说明）；DETR 一般**不做 NMS**。[Ultralytics Docs+1](https://docs.ultralytics.com/modes/predict/?utm_source=chatgpt.com)

```
// postproc_yolo_det.hpp
#pragma once
#include "interfaces.hpp"
class YoloDetPost final : public IPostprocessor {
public:
  bool run(const std::vector<TensorView>& raw, const LetterboxMeta& m, ModelOutput& out) override;
private:
  // decode + NMS；按 meta 反投影到原图坐标
};
// postproc_yolo_seg.hpp
#pragma once
#include "interfaces.hpp"
class YoloSegPost final : public IPostprocessor {
public:
  bool run(const std::vector<TensorView>& raw, const LetterboxMeta& m, ModelOutput& out) override;
private:
  // det_head(含mask系数) + proto(PxHm x Wm) → 合成掩码 → 裁剪/上采样/阈值化
};
// postproc_detr.hpp
#pragma once
#include "interfaces.hpp"
class DetrPost final : public IPostprocessor {
public:
  bool run(const std::vector<TensorView>& raw, const LetterboxMeta& m, ModelOutput& out) override;
private:
  // 取 softmax(logits) 与 boxes(0~1) → 反归一化 → 一般不做 NMS
};
```

------

# 媒体 I/O 模块

## Source（RTSP 可热切换，FFmpeg 解码）

FFmpeg 的**现代 API**是 `avcodec_send_packet/receive_frame`（解码）与 `avcodec_send_frame/receive_packet`（编码）；循环处理 `EAGAIN` 是**正常流程控制**（官方 Doxygen & 示例）。[ffmpeg.org+1](https://ffmpeg.org/doxygen/6.1/demux_decode_8c-example.html?utm_source=chatgpt.com)

```
// media/source.hpp
#pragma once
#include "core/utils.hpp"
struct IFrameSource { virtual ~IFrameSource()=default; virtual bool grab(Frame&)=0; };
struct ISwitchableSource : IFrameSource { virtual bool switch_to(const std::string& url)=0; };
// media/source_switchable_rtsp.hpp
#pragma once
#include "source.hpp"
#include <memory>
#include <mutex>

class RtspFfmpegImpl; // ffmpeg 细节隐藏在 .cpp
class SwitchableRtspSource final : public ISwitchableSource {
public:
  bool switch_to(const std::string& url) override; // 预热成功后原子替换
  bool grab(Frame& out) override;
private:
  std::mutex mu_;
  std::shared_ptr<RtspFfmpegImpl> cur_;
};
// media/source_switchable_rtsp.cpp
#include "source_switchable_rtsp.hpp"
// RtspFfmpegImpl: 打开 demux/decoder，循环 av_read_frame→send_packet→receive_frame；必要时 sws_scale → BGR
// switch_to(): 后台启动新 impl 预热 N 帧成功后，在锁内替换 cur_；异常返回 false
```

## Encoder（FFmpeg H.264 / x264 或 NVENC）

```
// media/encoder.hpp
#pragma once
#include "core/utils.hpp"
#include <vector>
struct IEncoder {
  struct EncodedChunk { std::vector<uint8_t> data; bool key=false; double pts_ms=0; };
  virtual ~IEncoder()=default;
  virtual bool encode(const Frame& in, std::vector<EncodedChunk>& out)=0;
};
// media/encoder_h264_ffmpeg.hpp
#pragma once
#include "encoder.hpp"
class FfmpegH264Encoder final : public IEncoder {
public:
  bool open(int w,int h,int fps,int bitrate_kbps,int gop,bool zerolatency);
  void close();
  bool encode(const Frame& in, std::vector<EncodedChunk>& out) override; // send_frame→loop receive_packet
};
```

> 低时延建议：`bframes=0`、`tune=zerolatency`、合理 GOP；流程参见官方 `encode_video.c`。[ffmpeg.org](https://ffmpeg.org/doxygen/5.1/encode__video_8c_source.html?utm_source=chatgpt.com)

## Transport（WHIP 发布）

**WHIP** 是 IETF 标准（RFC 9725），用于 WebRTC 上行；**WHEP** 为下行草案。你只需把编码后的媒体送入 WHIP 连接（内部是 WebRTC 传输）。[datatracker.ietf.org+1](https://datatracker.ietf.org/doc/rfc9725/?utm_source=chatgpt.com)

```
// media/transport.hpp
#pragma once
#include "encoder.hpp"
struct ITransport { virtual ~ITransport()=default; virtual bool send(const IEncoder::EncodedChunk&)=0; };
// media/transport_whip.hpp
#pragma once
#include "transport.hpp"
#include <string>
class WhipPublisher final : public ITransport {
public:
  explicit WhipPublisher(std::string whip_url): url_(std::move(whip_url)) {}
  bool connect(); // HTTP POST SDP-OFFER → 收 ANSWER（见 RFC 9725）
  bool send(const IEncoder::EncodedChunk& c) override; // 将编码流喂给 WebRTC Sender
  void close();   // HTTP DELETE 释放资源
private:
  std::string url_;
  // 内部封装 WebRTC Sender (SRTP/ICE/DTLS)；不展开
};
```

> 规范细节：WHIP 的 **POST/DELETE** 建立与释放资源的语义在 RFC 9725 中有明确描述；WHEP 文档/仓库也有操作示例。[RFC Editor+2IETF+2](https://www.rfc-editor.org/rfc/rfc9725.pdf?utm_source=chatgpt.com)

------

# EngineManager（全局推理引擎切换）

```
// core/engine_manager.hpp
#pragma once
#include <string>
#include <mutex>
#include <memory>
#include "analyzer/interfaces.hpp"

struct EngineSpec { std::string ep="ort-cuda"; int device=0; /* trt_fp16, workspace_mb... */ };

class EngineManager {
public:
  EngineSpec current() const { std::lock_guard<std::mutex> lk(mu_); return spec_; }
  void set(const EngineSpec& e){ std::lock_guard<std::mutex> lk(mu_); spec_ = e; } // 真实实现：预热→A/B切换
  std::shared_ptr<IModelSession> make_session(const std::string& onnx, int in_w, int in_h) const;
private:
  mutable std::mutex mu_;
  EngineSpec spec_;
};
```

> ONNX Runtime 的 EP 工作方式、CUDA/TRT EP 文档与 IOBinding 见官方资料。[ONNX Runtime+3ONNX Runtime+3ONNX Runtime+3](https://onnxruntime.ai/docs/execution-providers/?utm_source=chatgpt.com)

------

# Pipeline / Builder / TrackManager

```
// core/factories.hpp  —— 显式工厂（组合根注入具体实现）
#pragma once
#include <functional>
#include "analyzer/analyzer.hpp"
#include "media/source.hpp"
#include "media/encoder.hpp"
#include "media/transport.hpp"

struct SourceCfg{ /* rtsp opts … */ };
struct FilterCfg{ std::string task; std::string onnx; int in_w=640,in_h=640; };
struct EncoderCfg{ int w,h,fps,bitrate_kbps,gop; bool zerolatency=true; };
struct TransportCfg{ std::string whip_url; };

struct Factories {
  std::function<std::shared_ptr<ISwitchableSource>(const SourceCfg&)>  make_source;
  std::function<std::shared_ptr<Analyzer>(const FilterCfg&)>            make_filter;
  std::function<std::shared_ptr<IEncoder>(const EncoderCfg&)>           make_encoder;
  std::function<std::shared_ptr<ITransport>(const TransportCfg&)>       make_transport;
};
// core/pipeline.hpp/.cpp
struct Profile { std::string id; std::string task; int out_w, out_h, fps, bitrate_kbps, gop; std::string whip_url; };

class Pipeline {
public:
  Pipeline(std::shared_ptr<ISwitchableSource> s,
           std::shared_ptr<Analyzer> a,
           std::shared_ptr<IEncoder> e,
           std::shared_ptr<ITransport> t);
  void start(); void stop();
  ISwitchableSource* source();  Analyzer* analyzer();
private:
  // 解码→分析→编码→发送 的线程循环；有界队列+“丢旧保新”
};
// core/pipeline_builder.hpp/.cpp
class PipelineBuilder {
public:
  PipelineBuilder(const Factories& F, EngineManager& E): F_(F), E_(E) {}
  std::shared_ptr<Pipeline> build(const std::string& stream, const Profile& p,
                                  const std::string& onnx, const std::string& init_url);
private:
  const Factories& F_; EngineManager& E_;
};
// core/track_manager.hpp/.cpp  —— 按需生产 + 动态切换
class TrackManager {
public:
  explicit TrackManager(PipelineBuilder& b);
  std::string subscribe(const std::string& stream,const Profile& p,
                        const std::string& onnx,const std::string& init_url);
  void unsubscribe(const std::string& stream,const std::string& profile);
  void reap_idle(int idle_ms);

  // 动态切换
  bool switch_source(const std::string& stream,const std::string& prof,const std::string& url);
  bool switch_model (const std::string& stream,const std::string& prof,const std::string& onnx, EngineManager&);
  bool switch_task  (const std::string& stream,const std::string& prof,const std::string& task);
  bool set_params   (const std::string& stream,const std::string& prof,std::shared_ptr<AnalyzerParams> p);
};
```

------

# 组合根（把实现类绑定到工厂）

```
// composition_root.cpp
#include "core/factories.hpp"
#include "core/engine_manager.hpp"
#include "analyzer/preproc_letterbox_cpu.hpp"
#include "analyzer/ort_session.hpp"
#include "analyzer/postproc_yolo_det.hpp"
#include "analyzer/postproc_yolo_seg.hpp"
#include "analyzer/postproc_detr.hpp"
#include "media/source_switchable_rtsp.hpp"
#include "media/encoder_h264_ffmpeg.hpp"
#include "media/transport_whip.hpp"

Factories build_factories(EngineManager& engine) {
  Factories F;
  F.make_source = [](const SourceCfg&){
    auto s = std::make_shared<SwitchableRtspSource>();
    return s;
  };
  F.make_filter = [&engine](const FilterCfg& c){
    auto A = std::make_shared<Analyzer>();
    auto pre = std::make_shared<LetterboxPreCPU>(c.in_w, c.in_h);   // 或 CUDA 版
    auto ses = std::make_shared<OrtSession>(c.onnx, engine.current(), c.in_w, c.in_h);
    std::shared_ptr<IPostprocessor> post;
    if (c.task=="det") post = std::make_shared<YoloDetPost>();
    else if (c.task=="seg") post = std::make_shared<YoloSegPost>();
    else if (c.task=="detr") post = std::make_shared<DetrPost>();
    A->swap_pre(pre); A->swap_ses(ses); A->swap_post(post);
    return A;
  };
  F.make_encoder = [](const EncoderCfg& e){
    auto enc = std::make_shared<FfmpegH264Encoder>();
    enc->open(e.w,e.h,e.fps,e.bitrate_kbps,e.gop,e.zerolatency);
    return enc;
  };
  F.make_transport = [](const TransportCfg& t){
    auto tx = std::make_shared<WhipPublisher>(t.whip_url);
    tx->connect();
    return tx;
  };
  return F;
}
```

------

# REST 控制面（`server/rest.cpp` 简述）

- `POST /subscribe` → 调用 `TrackManager::subscribe`（首次创建并 `start()`），返回 `{"whep": "<your-sfu>/stream_profile/whep"}`
- `POST /unsubscribe` → `TrackManager::unsubscribe`
- `POST /source/switch` / `/model/switch` / `/task/switch` / `PATCH /model/params` / `POST /engine/set` → 直接转发到 `TrackManager` / `EngineManager`

------

# 实现时的关键参考点

- **FFmpeg send/receive**：官方 Doxygen 与示例展示了推荐流程；`EAGAIN` 表示内部缓冲需继续送/取，属正常。[ffmpeg.org+2ffmpeg.org+2](https://ffmpeg.org/doxygen/6.1/demux_decode_8c-example.html?utm_source=chatgpt.com)
- **ONNX Runtime**：
  - Execution Providers（CUDA / TensorRT / CPU）文档与工作机制：将子图下放给 EP 执行。[ONNX Runtime+2ONNX Runtime+2](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html?utm_source=chatgpt.com)
  - **IOBinding**：预先把输入/输出放上设备，Run 时不做隐式拷贝，是实时流降时延的关键。[ONNX Runtime+1](https://onnxruntime.ai/docs/performance/tune-performance/iobinding.html?utm_source=chatgpt.com)
- **协议**：WHIP（标准 RFC 9725）与 WHEP（IETF 草案 / 参考仓库）。[GitHub+3datatracker.ietf.org+3RFC Editor+3](https://datatracker.ietf.org/doc/rfc9725/?utm_source=chatgpt.com)
- **后处理**：
  - YOLO 推理参数（`conf`、`iou`）与 NMS 调优。[Ultralytics Docs+1](https://docs.ultralytics.com/modes/predict/?utm_source=chatgpt.com)
  - DETR 通常不做 NMS（集合预测/匈牙利匹配）。*（此处算法原理广泛见诸 DETR 论文/官方说明；实现上只做阈值与坐标反变换即可）。*